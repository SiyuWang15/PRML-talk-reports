\PassOptionsToPackage{english}{babel}
\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
%\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{amssymb}

\newcommand{\norm}{\mathcal N}
\newcommand{\mbf}{\mathbf}
\newcommand{\mrm}{\mathrm}
\newcommand{\mcal}{\mathcal}

\title{Reports on PRML Reading Talk}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Siyu Wang\\
  Department of Computer Science\\
  Tsinghua University\\
  \texttt{thuwangsy@gmail.com} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\section{Introduction to ML, Probability Basics}
\emph{PRML chap.1,2; MLAPP chap.1,2}

The key purpose of machine learning is to extract good \textbf{features} and \textbf{patterns} from \textbf{data}. To realize this purpose, we mainly use function fitting as the tool to solve this problem. In this section we will focus on a polynomial fitting problem, solving it from different angles and considering differences and relations among them. 

But we must keep in mind that, function fitting is only a mathematical tool, not our purpose.

Consider the following problem: given some observed points:
\begin{center}
$D=\{(x_1,t_1),(x_2,t_2), \cdots, (x_N,x_N)\}$
\end{center}
in which, $t = \sin(2\pi x)+\norm(0, \sigma^2)$. Then we want to find a polynomial function
\begin{equation}\label{eq1.1}
    t = y(x,\bf{w})= w_0+w_1x+w_2x^2+...+x_M x^M \tag{1.1}
\end{equation}
to fit these points, so when given new values of $x$, we can predict the corresponding $t$.

\subsection*{From function fitting angle}
We want to optimize the unknown parameter $\mrm{w}$ in equation.\ref{eq1.1} by minimize the \emph{error function}
\begin{equation}\label{eq1.2}
    E(\mrm w) = \frac12\sum_{n=1}^N\{y(x_n, \mrm w)-t_n\}^2.\tag{1.2}
\end{equation}
So we can set the error function's derivative with respect to $\textbf{w}$ to zero and easily get the optimal parameter $\mbf{w}^*$. 

To reduce the over-fitting problem, we may make a little change about the error function by adding a regularization item:
\begin{equation}\label{eq1.3}
    E(\mrm w) = \frac12\sum_{n=1}^N\{y(x_n, \mrm w)-t_n\}^2 + \frac{\lambda}{2}\|\mrm w\|^2. \tag{1.3}
\end{equation}

\subsection*{From probabilistic angle}
Here, we shall assume that, given the value of $x$, the corresponding value of $t$ has a Gaussian distribution with a mean equal to the value $y(x,\mrm{w})$ and a fixed variance $\beta$. So we have 
\begin{equation}
    p(t|x, \mrm w, \beta) = \norm(y(x, \mrm w), \beta^{-1}) \tag{1.4}
\end{equation}

\subsubsection*{MLE: maximum likelihood estimation}
Given $N$ input value $\mbf{x} = (x_1,\cdots, x_N)^T$ and their corresponding target value $\mbf t= (t_1, \cdots, t_N)^T$, we have the likelihood function with the form as 
\begin{equation}
    p(\mbf t|\mbf x, \mrm w, \beta) = \prod_{n=1}^N\norm(t_n|y(x_n,\mrm w),\beta^{-1}). \tag{1.5}
\end{equation}
It's convenient to maximize the logarithm of the likelihood function
\begin{equation}
    \ln p(\mbf t|\mbf x, \mrm w, \beta) = -\frac{\beta}{2}\sum_{n=1}^N\{y(x_n,\mrm w)-t_n\}^2+\frac N2\ln\beta-\frac N2\ln(2\pi). \tag{1.6}
\end{equation}
So when determining $\mbf w$, maximizing likelihood function is equal to minimizing the error function in equation.\ref{eq1.2}. And we obtain
\begin{gather}
    \frac1{\beta_{ML}} = \frac1N\sum_{n=1}^N\{y(x_n,\mrm w_{ML})-t_n\}^2. \tag{1.7}
\end{gather}
So we get a probability distribution of $t$ when given a new value of $x$
\begin{equation}
    p(t|x,\mrm w_{ML},\beta_{ML}) = \norm(t|y(x,\mrm w_{ML},\beta_{ML}) \tag{1.8}
\end{equation}

\subsubsection*{MAP: maximum a posteriori estimation}
Now we take a step towards a more Bayesian approach and introduce a prior distribution over the polynomial coefficient $\mrm w$ and for simplicity, consider a Gaussian distribution of the form 
\begin{equation}
    p(\mrm w|\alpha) = \norm(\mrm w|0,\alpha^{-1}\mrm I) = (\frac\alpha{2\pi})^{(M+1)/2}\exp\{-\frac\alpha2\|w\|^2\} \tag{1.9}
\end{equation}
where $\alpha$ is the precision of the distribution and $M+1$ is the total number of elements in the vector $\mbf w$. Using Bayes' theorem, we can get the posterior distribution for $\mbf w$, which is proportional to the product of the prior distribution and the likelihood function 
\begin{equation}
    p(\mrm w| \mbf x, \mbf t,\alpha, \beta) \propto p(\mbf t|\mrm w, \mbf x, \beta)p(\mrm w|\alpha).  \tag{1.10}
\end{equation}
And finally we find that the maximum of the posterior is given by the minimum of 
\begin{equation}
    \frac\beta2\sum_{n=1}^N\{y(x_n,\mrm w) -t_n\}^2+\frac\alpha2\mrm w^T\mrm w. \tag{1.11}
\end{equation}
Thus we see that maximizing the posterior distribution is equivalent to minimizing the regularized sum-of-square error function in the form (\ref{eq1.3})

\subsection*{Inference and decision}
Here, we come to a classification problem and we have three different approaches to doing inference and decision.
\begin{enumerate}
    \item First solve $p(\mrm{x}|\mathcal C_k)$ as well as $p(\mathcal C_k$ for each class individually, then figure out $p(\mathcal C_k|\mrm x) = \frac{p(\mathrm x|\mathcal C_k)p(\mathcal C_k)}{p(\mrm x)}$ and $p(\mrm x)$ can be gotten from $p(\mrm x) = \sum_kp(\mrm x|\mathcal C_k)p(\mathcal C_k)$. This is equivalent to model the joint distribution $p(\mrm x, \mathcal C_k)$
  \item Determine posterior class probabilities $p(\mathcal C_k|\mrm x)$  and then use decision theory.  \emph{discriminative model}.
  \item Find $f(\mathrm x)$ which maps $\mrm{x}$ directly to a class label.
\end{enumerate}
The first approach is too time-consuming and demanding, while the last one is simplest but not so robust as approach.2 because we can see a lot of information from the posterior probabilities.

\subsection*{For others}
Other than the main ideas and problems discussed above, there are still some other small items here: \emph{Gaussian probability distribution, exponential family, students' t distribution, conjugate prior, } and so on. For more details about these, we can refer to the original book. 

\section{Statistics, Information Theory Basics}
\emph{MLAPP Chap.5,6}
\subsection*{Bayesian Statistics}
Bayes' theorem: 
\begin{equation}
    p(\theta|D)=\frac{p(D|\theta)p(\theta)}{p(D)}
\end{equation}
Priors are assumptions about values of parameters and we can choose an appropriate prior from many different types: \emph{conjugate prior, uninformative prior, robust prior, Jeffreys prior}. 
\subsubsection*{Jeffreys prior}
Given a fixed likelihood $p(\mrm x|\phi)$ where $\mrm x$ represents the data, $\phi$ represents parameters. Then we define Fisher information as the form:
\begin{equation}
    I_{\phi}(\phi)=-\mrm E_{\mrm X\sim p(\mrm x|\phi)}[(\frac{\mrm d\log p(\mrm X|\phi)}{\mrm d\phi})^2]
\end{equation}
and Jeffreys prior is 
\begin{equation}
    p(\phi)\propto(I_{\phi}(\phi))^{1/2}
\end{equation}
An important property of Jeffreys prior is that if we assume $\theta=h(\phi)$, then the prior of $\theta$ is also Jeffreys prior:
\begin{equation}
    p(\theta)\propto (I_{\theta}(\theta))^{1/2}
\end{equation}

And we need to notice that the mixtures of conjugate priors is still a conjugate prior.

\subsubsection*{Hierarchical Bayes}
We can change a 1-level model with the form as:
\begin{equation}
    \theta \rightarrow D
\end{equation}
into a 2-level one by adding a parameter over the prior of $\theta$, then we have
\begin{equation}
    \eta\rightarrow\theta\rightarrow D
\end{equation}
If we apply maximum likelihood respectively to these two models, we can obtain
\begin{gather}
    \mrm{1-level model: }\theta^*=\arg\max_{\theta}p(D|\theta) \\
    \mrm{2-level model: }\eta^*=\arg\max_\eta p(D|\eta)=\arg\max_{\eta}[\int p(D|\theta)p(\theta|\eta)\mrm d\theta]
\end{gather}.
The maximum likelihood for 2-level model is also called \textbf{Empirical Bayes}.
\subsubsection*{Bayesian decision theory}

\subsection*{Frequentist Statistics}
In this part, we see inference from another angle, avoiding treating parameters like random variables, avoiding the use of priors and starting from sampling distribution. 

Given a distribution $p(x|\theta^*)$ where $\theta$ is fixed and we have data $D=(x_1, x_2, \dots, x_n)$ where $x_i\sim p(x|\theta^*)$ and are i.i.d. We want to obtain an estimator $\hat{\theta}=\delta(D)$, which is a function of data to estimate the true parameters $\theta$. 

\section{Linear models(1)}
\emph{MLAPP Chap.7; PRML Chap.3}

In this section, we mainly want to solve the regression problem using a linear model from MLE angle and MAP angle. 
\textbf{Problem setting:} Consider outputs is a linear combination of the inputs 
\begin{equation}
    y(\mbf x) = \mbf w^T\mbf x+\epsilon = \sum_{j=1}^{d}w_jx_j+\epsilon.
\end{equation}
And we can rewrite the model in a probabilistic form:
\begin{equation}
    p(y|\mbf x, \theta) = \norm(y|\mu(\mbf x),\sigma^2(\mbf x)) = \norm(y|\mbf w^T\mbf x, \sigma^2(\mbf x))
\end{equation}
To extend this model linear model, we introduce basis functions, and now the model is 
\begin{equation}
    p(y|\mbf x,\theta) = \norm(y|\mbf w^T\phi(\mbf x),\sigma^2(\mbf x))
\end{equation}
where $\phi$ is a set of functions and $\phi(\mbf x) = (\phi_1(\mbf x), \dots, \phi_M\mbf x)$. And in most cases we assume that $\sigma^2(\mbf x) = \sigma^2$
\subsection*{Maximum Likelihood Estimator}
In this view, we want to find a set of parameters $\theta$ so that likelihood is maximized:
\begin{equation}
    \hat{\theta}=\arg\max_{\theta}\log p(\mathcal D|\theta). 
\end{equation}
And we usually consider the inputs and outputs are i.i.d, so we can obtain
\begin{equation}
    L(\theta) = \log p(\mcal D|\theta)=\log p(\mbf y|\mbf x, \theta) =\sum_{i=1}^{D}\log p(y_i|\mbf x_i,\theta).
\end{equation}
Then we replace $p(y_i|\mbf x_i, \theta)$ by Gaussian distribution, set the derivative with respect to $\mbf w$ to zero and we can obtain
\begin{equation}
    \hat{\mbf w}_{\mrm{MLE}} = (\mbf X^T\mbf X)^{-1}\mbf X^T\mbf y.
\end{equation}
Ans we can understand this method from geometry view. 

There is usually a serious over-fitting problem in MLE so we introduce a regularization item in the object function:
\begin{equation}
    J(\mbf w) = \frac{1}{N}\sum_{i=1}^{N}(y_i-\mbf w^T \mbf x_i)^2+\lambda\|\mbf w\|^2,
\end{equation}
and 
\begin{equation}
    \hat{\mbf w}_{\mrm{ridge}} = (\lambda \mrm I_d +\mbf X^T\mbf X)^{-1}\mbf X^T\mbf y.
\end{equation}

\subsection*{Maximum A Posterior -- Bayesian Linear Regression}
Using Bayes' theorem, we obtain
\begin{equation}
    p(\theta|\mcal D) = \frac{p(\mcal D|\theta)p(\theta)}{p(\mcal D)} \propto p(\mcal D|\theta)p(\theta)
\end{equation}
The conjugate prior corresponding to Gaussian likelihood is still Gaussian, so we assume the prior of $\mbf w$ has a Gaussian form:
\begin{equation}
    p(\mbf w) = \norm (\mbf w|\mbf w_0,\mbf V_0).
\end{equation}
So 
\begin{equation}
    p(\mbf w|\mbf X,\mbf y, \sigma^2)\propto \norm(\mbf w|\mbf w_0,\mbf V_0)\norm(\mbf y|\mbf X,\mbf w,\sigma^2\mbf I_N) = \norm(\mbf w|\mbf w_N,\mbf V_N)
\end{equation}
And if we choose $\mbf V_0=\alpha^{-1}\mbf I$ and $\alpha\rightarrow 0$, we will see $\hat{\mbf w}_{\mrm{MLE}} = \mbf w_N$

\subsubsection*{Bayesian Model Selection}
\begin{equation}
    \mrm{Bayesian factor}:=\frac{p(\mcal D|\mcal M_i)}{p(\mcal D|\mcal M_j)}
\end{equation}
A model $\mcal M_i$ is controlled by a set of parameters $\mbf w$, and 
\begin{equation}
    p(\mcal D|\mcal M_i) = \int p(\mcal D|\mbf w,\mcal M_i)p(\mbf w|\mcal M_i)\mrm d\mbf w.
\end{equation}

\section{Linear models(2)}
\emph{MLAPP Chap.8; PRML Chap.4}
\subsubsection*{Logistic Regression}
Using Bayes Theorem we have 
\begin{equation}
    p(C_1|\mrm x) = \frac{p(\mrm x|C_1)p(C_1)}{p(\mrm x)} = \frac{p(\mrm x|C_1)p(C_1)}{p(\mrm x|C_1)p(C_1)+p(\mrm x|C_2)p(C_2)}=\sigma(a)
\end{equation}
where $a =\log\frac{p(\mrm x|C_2)p(C_2)}{p(\mrm x|C_1)p(C_1)}$.

Suppose $p(\mrm x|C_k)$ is  Gaussian, then we can obtain $a = \mrm w^T\mrm x+\mrm w_0$ and this is very simple to compute $p(C_1|\mrm x)$.

\textbf{Maximum Conditional Likelihood Estimator}.\newline
\begin{equation}
    \hat{\mrm w} = \arg\max_{\mrm x}\prod_{i=1}^{N}p(y_i|\mrm x_i,\mrm w).
\end{equation}
Take the logarithm form to be likelihood function and we have 
\begin{equation}
    L(\mrm w) = -\log\prod_{i=1}^{N}p(y_i|\mrm x_i,\mrm w)=-\sum_{i=1}^{N}\log(p(y_i|\mrm x_i,\mrm w))
\end{equation}
Which is not solvable analytically, so we have no closed-form solution and we need some descent algorithms to minimize $L(\mrm w)$.
\textbf{Gradient descent}
In each iteration, we update the parameter by taking a step backwards the direction which the gradient points to
\begin{equation}
    \mrm w^{(\tau+1)} = \mrm w^{\tau}-\eta \nabla_{\mrm w}L(\mrm w)
\end{equation}

\subsubsection*{Bayesian Logistic Regression}
Estimate the posterior of parameters $\mrm w$ rather than just a point estimation.

\textbf{Generative model}: learn a model of the \textbf{joint probability}, $p(x, y)$, of the inputs $x$ and the label $y$, and make their predictions by using Bayes rules to calculate $p(ylx)$, and then picking the most likely label $y$.\newline
\textbf{Discriminative model}: learn the posterior $p(ylx)$ directly, or learn a direct map from inputs $x$ to the class labels.
\section{Kernels}
\emph{MLAPP Chap.14; PRML Chap.6,7}

\section{Graphical models--mlapp chap.10, 19. prml chap 8.1-8.3}

\section{Latent variable model, clustering and EM. prml chap.9 mlapp chap.11, 25}

\section{Continuous latent variables. prml chap 12. mlapp.12,13}

\section{Exact inference--mlapp chap.20. prml chap 8.4}

\section{Variational inference--prml chap10. mlapp chap.21,22}

\section{Monte Carlo-- prml chap.11. mlapp chap.23,24}

\section{Sequential data--prml chap.13. mlapp chap.17-18.}

\section{Gaussian Process--mlapp chap15}

\section{Neural network--mlapp chap 16. prml chap5.}

\section{Deep learning--mlapp chap 28.}

\section{Combining models. prml chap 14}
\end{document}